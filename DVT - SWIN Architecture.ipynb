{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cee69729-5edc-4aaf-8f66-a44dc7e0a186",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "005c7df8-9a5a-4420-bee6-70b81cad2cb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "597e8728-722d-4b14-9466-1c9f5eb78005",
   "metadata": {},
   "outputs": [],
   "source": [
    "from swin_transformer_final import SwinTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c786f0ed-44a8-4631-9d65-f0466263e832",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-12 17:56:17.701035: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
      "2024-04-12 17:56:17.701050: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2024-04-12 17:56:17.701054: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2024-04-12 17:56:17.701079: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-04-12 17:56:17.701091: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "model = SwinTransformer(model_name='swin_tiny_224', num_classes=5, include_top=True, pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3834ac0-ba04-4740-90d3-60f57ba58568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"swin_tiny_224\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " patch_embed (PatchEmbed)    multiple                  4896      \n",
      "                                                                 \n",
      " dropout (Dropout)           multiple                  0         \n",
      "                                                                 \n",
      " sequential_4 (Sequential)   (None, 49, 768)           27762626  \n",
      "                                                                 \n",
      " norm (LayerNormalization)   multiple                  1536      \n",
      "                                                                 \n",
      " global_average_pooling1d (  multiple                  0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " head (Dense)                multiple                  3845      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27772903 (106.90 MB)\n",
      "Trainable params: 27523199 (104.99 MB)\n",
      "Non-trainable params: 249704 (1.91 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e540e66-b95e-4f06-8abf-e2aff6954a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "datagen = ImageDataGenerator(rescale = 1./255, \n",
    "                             shear_range = 0.2, \n",
    "                             zoom_range = 0.3, \n",
    "                             horizontal_flip = True, \n",
    "                             validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45bcea39-5089-493b-a05a-9f2972f320bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (224,224)\n",
    "batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ec47df7-52db-4474-b1ca-b8edb2ca5890",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6330ee09-38b4-44d3-8c7a-196e1bf56087",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'Lung_Disease_Dataset/train'\n",
    "classes = os.listdir(data_path)\n",
    "classes = [i for i in classes if not i.startswith('.')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8cde855-9fe8-4d81-ac98-8aa68ea7cc6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4845 images belonging to 5 classes.\n",
      "Found 1209 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = datagen.flow_from_directory(data_path, \n",
    "                                            target_size=image_size, \n",
    "                                            batch_size=batch_size, \n",
    "                                            class_mode = 'categorical',\n",
    "                                            subset='training')\n",
    "\n",
    "valid_datagen = datagen.flow_from_directory(data_path, \n",
    "                                            target_size=image_size,\n",
    "                                            batch_size=batch_size,  \n",
    "                                            class_mode='categorical', \n",
    "                                            subset='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d0e88e2-da48-481e-9e80-7675a0b304af",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "63581533-3c1d-4263-97ea-03771d4ff556",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 done!\n",
      "Batch 1 done!\n",
      "Batch 2 done!\n",
      "Batch 3 done!\n",
      "Batch 4 done!\n",
      "Batch 5 done!\n",
      "Batch 6 done!\n",
      "Batch 7 done!\n",
      "Batch 8 done!\n",
      "Batch 9 done!\n",
      "Batch 10 done!\n",
      "Batch 11 done!\n",
      "Batch 12 done!\n",
      "Batch 13 done!\n",
      "Batch 14 done!\n",
      "Batch 15 done!\n",
      "Batch 16 done!\n",
      "Batch 17 done!\n",
      "Batch 18 done!\n",
      "Batch 19 done!\n",
      "Batch 20 done!\n",
      "Batch 21 done!\n",
      "Batch 22 done!\n",
      "Batch 23 done!\n",
      "Batch 24 done!\n",
      "Batch 25 done!\n",
      "Batch 26 done!\n",
      "Batch 27 done!\n",
      "Batch 28 done!\n",
      "Batch 29 done!\n",
      "Batch 30 done!\n",
      "Batch 31 done!\n",
      "Batch 32 done!\n",
      "Batch 33 done!\n",
      "Batch 34 done!\n",
      "Batch 35 done!\n",
      "Batch 36 done!\n",
      "Batch 37 done!\n",
      "Batch 38 done!\n",
      "Batch 39 done!\n",
      "Batch 40 done!\n",
      "Batch 41 done!\n",
      "Batch 42 done!\n",
      "Batch 43 done!\n",
      "Batch 44 done!\n",
      "Batch 45 done!\n",
      "Batch 46 done!\n",
      "Batch 47 done!\n",
      "Batch 48 done!\n",
      "Batch 49 done!\n",
      "Batch 50 done!\n",
      "Batch 51 done!\n",
      "Batch 52 done!\n",
      "Batch 53 done!\n",
      "Batch 54 done!\n",
      "Batch 55 done!\n",
      "Batch 56 done!\n",
      "Batch 57 done!\n",
      "Batch 58 done!\n",
      "Batch 59 done!\n",
      "Batch 60 done!\n",
      "Batch 61 done!\n",
      "Batch 62 done!\n",
      "Batch 63 done!\n",
      "Batch 64 done!\n",
      "Batch 65 done!\n",
      "Batch 66 done!\n",
      "Batch 67 done!\n",
      "Batch 68 done!\n",
      "Batch 69 done!\n",
      "Batch 70 done!\n",
      "Batch 71 done!\n",
      "Batch 72 done!\n",
      "Batch 73 done!\n",
      "Batch 74 done!\n",
      "Batch 75 done!\n",
      "Batch 76 done!\n",
      "Batch 77 done!\n",
      "Batch 78 done!\n",
      "Batch 79 done!\n",
      "Batch 80 done!\n",
      "Batch 81 done!\n",
      "Batch 82 done!\n",
      "Batch 83 done!\n",
      "Batch 84 done!\n",
      "Batch 85 done!\n",
      "Batch 86 done!\n",
      "Batch 87 done!\n",
      "Batch 88 done!\n",
      "Batch 89 done!\n",
      "Batch 90 done!\n",
      "Batch 91 done!\n",
      "Batch 92 done!\n",
      "Batch 93 done!\n",
      "Batch 94 done!\n",
      "Batch 95 done!\n",
      "Batch 96 done!\n",
      "Batch 97 done!\n",
      "Batch 98 done!\n",
      "Batch 99 done!\n",
      "Batch 100 done!\n",
      "Batch 101 done!\n",
      "Batch 102 done!\n",
      "Batch 103 done!\n",
      "Batch 104 done!\n",
      "Batch 105 done!\n",
      "Batch 106 done!\n",
      "Batch 107 done!\n",
      "Batch 108 done!\n",
      "Batch 109 done!\n",
      "Batch 110 done!\n",
      "Batch 111 done!\n",
      "Batch 112 done!\n",
      "Batch 113 done!\n",
      "Batch 114 done!\n",
      "Batch 115 done!\n",
      "Batch 116 done!\n",
      "Batch 117 done!\n",
      "Batch 118 done!\n",
      "Batch 119 done!\n",
      "Batch 120 done!\n",
      "Batch 121 done!\n",
      "Batch 122 done!\n",
      "Batch 123 done!\n",
      "Batch 124 done!\n",
      "Batch 125 done!\n",
      "Batch 126 done!\n",
      "Batch 127 done!\n",
      "Batch 128 done!\n",
      "Batch 129 done!\n",
      "Batch 130 done!\n",
      "Batch 131 done!\n",
      "Batch 132 done!\n",
      "Batch 133 done!\n",
      "Batch 134 done!\n",
      "Batch 135 done!\n",
      "Batch 136 done!\n",
      "Batch 137 done!\n",
      "Batch 138 done!\n",
      "Batch 139 done!\n",
      "Batch 140 done!\n",
      "Batch 141 done!\n",
      "Batch 142 done!\n",
      "Batch 143 done!\n",
      "Batch 144 done!\n",
      "Batch 145 done!\n",
      "Batch 146 done!\n",
      "Batch 147 done!\n",
      "Batch 148 done!\n",
      "Batch 149 done!\n",
      "Batch 150 done!\n",
      "Batch 151 done!\n",
      "Batch 152 done!\n",
      "Batch 153 done!\n",
      "Batch 154 done!\n",
      "Batch 155 done!\n",
      "Batch 156 done!\n",
      "Batch 157 done!\n",
      "Batch 158 done!\n",
      "Batch 159 done!\n",
      "Batch 160 done!\n",
      "Batch 161 done!\n",
      "Batch 162 done!\n",
      "Batch 163 done!\n",
      "Batch 164 done!\n",
      "Batch 165 done!\n",
      "Batch 166 done!\n",
      "Batch 167 done!\n",
      "Batch 168 done!\n",
      "Batch 169 done!\n",
      "Batch 170 done!\n",
      "Batch 171 done!\n",
      "Batch 172 done!\n",
      "Batch 173 done!\n",
      "Batch 174 done!\n",
      "Batch 175 done!\n",
      "Batch 176 done!\n",
      "Batch 177 done!\n",
      "Batch 178 done!\n",
      "Batch 179 done!\n",
      "Batch 180 done!\n",
      "Batch 181 done!\n",
      "Batch 182 done!\n",
      "Batch 183 done!\n",
      "Batch 184 done!\n",
      "Batch 185 done!\n",
      "Batch 186 done!\n",
      "Batch 187 done!\n",
      "Batch 188 done!\n",
      "Batch 189 done!\n",
      "Batch 190 done!\n",
      "Batch 191 done!\n",
      "Batch 192 done!\n",
      "Batch 193 done!\n",
      "Batch 194 done!\n",
      "Batch 195 done!\n",
      "Batch 196 done!\n",
      "Batch 197 done!\n",
      "Batch 198 done!\n",
      "Batch 199 done!\n",
      "Batch 200 done!\n",
      "Batch 201 done!\n",
      "Batch 202 done!\n",
      "Batch 203 done!\n",
      "Batch 204 done!\n",
      "Batch 205 done!\n",
      "Batch 206 done!\n",
      "Batch 207 done!\n",
      "Batch 208 done!\n",
      "Batch 209 done!\n",
      "Batch 210 done!\n",
      "Batch 211 done!\n",
      "Batch 212 done!\n",
      "Batch 213 done!\n",
      "Batch 214 done!\n",
      "Batch 215 done!\n",
      "Batch 216 done!\n",
      "Batch 217 done!\n",
      "Batch 218 done!\n",
      "Batch 219 done!\n",
      "Batch 220 done!\n",
      "Batch 221 done!\n",
      "Batch 222 done!\n",
      "Batch 223 done!\n",
      "Batch 224 done!\n",
      "Batch 225 done!\n",
      "Batch 226 done!\n",
      "Batch 227 done!\n",
      "Batch 228 done!\n",
      "Batch 229 done!\n",
      "Batch 230 done!\n",
      "Batch 231 done!\n",
      "Batch 232 done!\n",
      "Batch 233 done!\n",
      "Batch 234 done!\n",
      "Batch 235 done!\n",
      "Batch 236 done!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m labels \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      5\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_datagen\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[1;32m      7\u001b[0m     images\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mlist\u001b[39m(train_datagen[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m      8\u001b[0m     labels\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mlist\u001b[39m(train_datagen[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m]))\n",
      "File \u001b[0;32m~/anaconda3/envs/tfmetal/lib/python3.10/site-packages/keras/src/preprocessing/image.py:156\u001b[0m, in \u001b[0;36mIterator.__next__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tfmetal/lib/python3.10/site-packages/keras/src/preprocessing/image.py:168\u001b[0m, in \u001b[0;36mIterator.next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    165\u001b[0m     index_array \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex_generator)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m# The transformation of images is not under thread lock\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;66;03m# so it can be done in parallel\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_batches_of_transformed_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex_array\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tfmetal/lib/python3.10/site-packages/keras/src/preprocessing/image.py:370\u001b[0m, in \u001b[0;36mBatchFromFilesMixin._get_batches_of_transformed_samples\u001b[0;34m(self, index_array)\u001b[0m\n\u001b[1;32m    368\u001b[0m filepaths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepaths\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(index_array):\n\u001b[0;32m--> 370\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mimage_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_img\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilepaths\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolor_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m        \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_aspect_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeep_aspect_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    377\u001b[0m     x \u001b[38;5;241m=\u001b[39m image_utils\u001b[38;5;241m.\u001b[39mimg_to_array(img, data_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_format)\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;66;03m# Pillow images should be closed after `load_img`,\u001b[39;00m\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;66;03m# but not PIL images.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tfmetal/lib/python3.10/site-packages/keras/src/utils/image_utils.py:479\u001b[0m, in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[1;32m    477\u001b[0m             img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mresize(width_height_tuple, resample, box\u001b[38;5;241m=\u001b[39mcrop_box)\n\u001b[1;32m    478\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 479\u001b[0m             img \u001b[38;5;241m=\u001b[39m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwidth_height_tuple\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/anaconda3/envs/tfmetal/lib/python3.10/site-packages/PIL/Image.py:2164\u001b[0m, in \u001b[0;36mImage.resize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   2160\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m   2162\u001b[0m size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(size)\n\u001b[0;32m-> 2164\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m box \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2166\u001b[0m     box \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize\n",
      "File \u001b[0;32m~/anaconda3/envs/tfmetal/lib/python3.10/site-packages/PIL/ImageFile.py:291\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[1;32m    290\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[0;32m--> 291\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "images = []\n",
    "labels = []\n",
    "\n",
    "\n",
    "i = 0\n",
    "while(next(train_datagen)):\n",
    "    images.extend(list(train_datagen[0][0]))\n",
    "    labels.extend(list(train_datagen[0][1]))\n",
    "    print(f\"Batch {i} done!\")\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b89ff486-73e7-44d5-bcc7-323fdab8042d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7584"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c34249cd-bbe8-4ab4-aeb1-3e571fd65c71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7584"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bab6f26f-b2ea-4c0a-abbb-5695b7de4ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.array(images)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "30b12d67-a68c-4e86-aee4-8d51d4b1238b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7584, 224, 224, 3)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3a831c5d-39fe-40fa-9956-fae5a91ebbd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7584, 5)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9340138d-a98f-4bdf-b8ea-bcbd3dd0056e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-03 16:22:21.422962: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201/237 [========================>.....] - ETA: 1:11 - loss: 2.0845e-04 - accuracy: 0.2030"
     ]
    }
   ],
   "source": [
    "model.fit(images, labels, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da94e888-938a-4360-b71d-8f7199dcc2e9",
   "metadata": {},
   "source": [
    "## Implementation 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53d0e004-bf62-4194-a427-6a760054b1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)        [(None, 224, 224, 1)]        0         []                            \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)        [(None, 224, 224, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)             (None, 224, 224, 64)         640       ['input_2[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)           (None, 224, 224, 64)         1792      ['input_3[0][0]']             \n",
      "                                                                                                  \n",
      " SwinTransformer (SwinTrans  (None, 768)                  2786275   ['conv2d[0][0]',              \n",
      " formerModel)                                             4          'conv2d_1[0][0]']            \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 1536)                 0         ['SwinTransformer[0][0]',     \n",
      "                                                                     'SwinTransformer[1][0]']     \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 2)                    3074      ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 27868260 (107.26 MB)\n",
      "Trainable params: 27618556 (105.36 MB)\n",
      "Non-trainable params: 249704 (1.91 MB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, Dense, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from swin_transformer_final import SwinTransformerModel, CFGS\n",
    "\n",
    "def create_model(input_shape_vsi, input_shape_tda, num_classes, pretrained=True, swin_model_type = 'swin_tiny_224'):\n",
    "    # Inputs for VSI and TDA images\n",
    "    input_vsi = Input(shape=input_shape_vsi)\n",
    "    input_tda = Input(shape=input_shape_tda)\n",
    "\n",
    "    # Initial Convolutional layer for each input\n",
    "    conv_vsi = Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same')(input_vsi)\n",
    "    conv_tda = Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same')(input_tda)\n",
    "\n",
    "    # Load the Swin Transformer model configuration\n",
    "    swin_config = CFGS[swin_model_type]\n",
    "    swin_transformer_model = SwinTransformerModel(include_top=False, num_classes=num_classes, **swin_config)\n",
    "    model_name = swin_model_type\n",
    "\n",
    "    ###################################\n",
    "    if pretrained is True:\n",
    "        url = f'https://github.com/rishigami/Swin-Transformer-TF/releases/download/v0.1-tf-swin-weights/{model_name}.tgz'\n",
    "        pretrained_weights = tf.keras.utils.get_file(model_name, url, untar=True)\n",
    "    else:\n",
    "        pretrained_weights = pretrained\n",
    "    \n",
    "    # Load weights into the model if pretrained_weights is provided\n",
    "    if pretrained_weights:\n",
    "        if tf.io.gfile.isdir(pretrained_weights):\n",
    "            pretrained_weights = f'{pretrained_weights}/{model_name}.ckpt'\n",
    "        \n",
    "        \n",
    "    swin_transformer_model.load_weights(pretrained_weights)\n",
    "    #####################################\n",
    "\n",
    "    # Swin Transformer Encoder blocks for VSI and TDA images\n",
    "    swin_output_vsi = swin_transformer_model(conv_vsi)\n",
    "    swin_output_tda = swin_transformer_model(conv_tda)\n",
    "\n",
    "    # Concatenate the output features from the Swin Transformer encoders\n",
    "    concatenated = Concatenate()([swin_output_vsi, swin_output_tda])\n",
    "\n",
    "    # Classifier head\n",
    "    classifier_output = Dense(num_classes, activation='softmax')(concatenated)\n",
    "\n",
    "    # Create the Model\n",
    "    model = Model(inputs=[input_vsi, input_tda], outputs=classifier_output)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Define input shapes based on the preprocessed data sizes\n",
    "input_shape_vsi = (224, 224, 1)  # Replace with actual size\n",
    "input_shape_tda = (224, 224, 3)  # Replace with actual size\n",
    "num_classes = 2  # Healthy or Diseased\n",
    "\n",
    "# Create the model\n",
    "model = create_model(input_shape_vsi, input_shape_tda, num_classes)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "717d682b-4e7c-4f8e-9190-9829a07f0c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).norm.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).norm.beta\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).patch_embed.proj.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).patch_embed.proj.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).patch_embed.norm.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).patch_embed.norm.beta\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-0.blocks.layer_with_weights-1.attn_mask\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-0.downsample.reduction.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-0.downsample.norm.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-0.downsample.norm.beta\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-1.blocks.layer_with_weights-1.attn_mask\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-1.downsample.reduction.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-1.downsample.norm.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-1.downsample.norm.beta\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-1.attn_mask\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-3.attn_mask\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-5.attn_mask\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.downsample.reduction.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.downsample.norm.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.downsample.norm.beta\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-0.blocks.layer_with_weights-0.norm1.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-0.blocks.layer_with_weights-0.norm1.beta\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-0.blocks.layer_with_weights-0.attn.layers0/blocks0/attn/relative_position_bias_table\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-0.blocks.layer_with_weights-0.attn.relative_position_index\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-0.blocks.layer_with_weights-0.norm2.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-0.blocks.layer_with_weights-0.norm2.beta\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-0.blocks.layer_with_weights-1.norm1.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-0.blocks.layer_with_weights-1.norm1.beta\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-0.blocks.layer_with_weights-1.attn.layers0/blocks1/attn/relative_position_bias_table\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-0.blocks.layer_with_weights-1.attn.relative_position_index\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-0.blocks.layer_with_weights-1.norm2.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-0.blocks.layer_with_weights-1.norm2.beta\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-1.blocks.layer_with_weights-0.norm1.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-1.blocks.layer_with_weights-0.norm1.beta\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-1.blocks.layer_with_weights-0.attn.layers1/blocks0/attn/relative_position_bias_table\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-1.blocks.layer_with_weights-0.attn.relative_position_index\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-1.blocks.layer_with_weights-0.norm2.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-1.blocks.layer_with_weights-0.norm2.beta\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-1.blocks.layer_with_weights-1.norm1.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-1.blocks.layer_with_weights-1.norm1.beta\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-1.blocks.layer_with_weights-1.attn.layers1/blocks1/attn/relative_position_bias_table\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-1.blocks.layer_with_weights-1.attn.relative_position_index\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-1.blocks.layer_with_weights-1.norm2.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-1.blocks.layer_with_weights-1.norm2.beta\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-0.norm1.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-0.norm1.beta\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-0.attn.layers2/blocks0/attn/relative_position_bias_table\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-0.attn.relative_position_index\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-0.norm2.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-0.norm2.beta\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-1.norm1.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-1.norm1.beta\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-1.attn.layers2/blocks1/attn/relative_position_bias_table\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-1.attn.relative_position_index\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-1.norm2.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-1.norm2.beta\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-2.norm1.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-2.norm1.beta\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-2.attn.layers2/blocks2/attn/relative_position_bias_table\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-2.attn.relative_position_index\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-2.norm2.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-2.norm2.beta\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-3.norm1.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-3.norm1.beta\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-3.attn.layers2/blocks3/attn/relative_position_bias_table\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-3.attn.relative_position_index\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-3.norm2.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-3.norm2.beta\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-4.norm1.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-4.norm1.beta\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-4.attn.layers2/blocks4/attn/relative_position_bias_table\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-4.attn.relative_position_index\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-4.norm2.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-4.norm2.beta\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-5.norm1.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-5.norm1.beta\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-5.attn.layers2/blocks5/attn/relative_position_bias_table\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-5.attn.relative_position_index\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-5.norm2.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-5.norm2.beta\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-3.blocks.layer_with_weights-0.norm1.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-3.blocks.layer_with_weights-0.norm1.beta\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-3.blocks.layer_with_weights-0.attn.layers3/blocks0/attn/relative_position_bias_table\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-3.blocks.layer_with_weights-0.attn.relative_position_index\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-3.blocks.layer_with_weights-0.norm2.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-3.blocks.layer_with_weights-0.norm2.beta\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-3.blocks.layer_with_weights-1.norm1.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-3.blocks.layer_with_weights-1.norm1.beta\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-3.blocks.layer_with_weights-1.attn.layers3/blocks1/attn/relative_position_bias_table\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-3.blocks.layer_with_weights-1.attn.relative_position_index\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-3.blocks.layer_with_weights-1.norm2.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-3.blocks.layer_with_weights-1.norm2.beta\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-0.blocks.layer_with_weights-0.attn.qkv.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-0.blocks.layer_with_weights-0.attn.qkv.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-0.blocks.layer_with_weights-0.attn.proj.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-0.blocks.layer_with_weights-0.attn.proj.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-0.blocks.layer_with_weights-0.mlp.fc1.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-0.blocks.layer_with_weights-0.mlp.fc1.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-0.blocks.layer_with_weights-0.mlp.fc2.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-0.blocks.layer_with_weights-0.mlp.fc2.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-0.blocks.layer_with_weights-1.attn.qkv.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-0.blocks.layer_with_weights-1.attn.qkv.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-0.blocks.layer_with_weights-1.attn.proj.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-0.blocks.layer_with_weights-1.attn.proj.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-0.blocks.layer_with_weights-1.mlp.fc1.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-0.blocks.layer_with_weights-1.mlp.fc1.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-0.blocks.layer_with_weights-1.mlp.fc2.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-0.blocks.layer_with_weights-1.mlp.fc2.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-1.blocks.layer_with_weights-0.attn.qkv.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-1.blocks.layer_with_weights-0.attn.qkv.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-1.blocks.layer_with_weights-0.attn.proj.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-1.blocks.layer_with_weights-0.attn.proj.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-1.blocks.layer_with_weights-0.mlp.fc1.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-1.blocks.layer_with_weights-0.mlp.fc1.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-1.blocks.layer_with_weights-0.mlp.fc2.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-1.blocks.layer_with_weights-0.mlp.fc2.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-1.blocks.layer_with_weights-1.attn.qkv.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-1.blocks.layer_with_weights-1.attn.qkv.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-1.blocks.layer_with_weights-1.attn.proj.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-1.blocks.layer_with_weights-1.attn.proj.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-1.blocks.layer_with_weights-1.mlp.fc1.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-1.blocks.layer_with_weights-1.mlp.fc1.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-1.blocks.layer_with_weights-1.mlp.fc2.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-1.blocks.layer_with_weights-1.mlp.fc2.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-0.attn.qkv.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-0.attn.qkv.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-0.attn.proj.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-0.attn.proj.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-0.mlp.fc1.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-0.mlp.fc1.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-0.mlp.fc2.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-0.mlp.fc2.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-1.attn.qkv.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-1.attn.qkv.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-1.attn.proj.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-1.attn.proj.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-1.mlp.fc1.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-1.mlp.fc1.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-1.mlp.fc2.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-1.mlp.fc2.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-2.attn.qkv.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-2.attn.qkv.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-2.attn.proj.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-2.attn.proj.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-2.mlp.fc1.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-2.mlp.fc1.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-2.mlp.fc2.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-2.mlp.fc2.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-3.attn.qkv.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-3.attn.qkv.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-3.attn.proj.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-3.attn.proj.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-3.mlp.fc1.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-3.mlp.fc1.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-3.mlp.fc2.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-3.mlp.fc2.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-4.attn.qkv.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-4.attn.qkv.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-4.attn.proj.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-4.attn.proj.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-4.mlp.fc1.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-4.mlp.fc1.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-4.mlp.fc2.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-4.mlp.fc2.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-5.attn.qkv.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-5.attn.qkv.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-5.attn.proj.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-5.attn.proj.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-5.mlp.fc1.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-5.mlp.fc1.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-5.mlp.fc2.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-2.blocks.layer_with_weights-5.mlp.fc2.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-3.blocks.layer_with_weights-0.attn.qkv.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-3.blocks.layer_with_weights-0.attn.qkv.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-3.blocks.layer_with_weights-0.attn.proj.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-3.blocks.layer_with_weights-0.attn.proj.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-3.blocks.layer_with_weights-0.mlp.fc1.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-3.blocks.layer_with_weights-0.mlp.fc1.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-3.blocks.layer_with_weights-0.mlp.fc2.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-3.blocks.layer_with_weights-0.mlp.fc2.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-3.blocks.layer_with_weights-1.attn.qkv.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-3.blocks.layer_with_weights-1.attn.qkv.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-3.blocks.layer_with_weights-1.attn.proj.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-3.blocks.layer_with_weights-1.attn.proj.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-3.blocks.layer_with_weights-1.mlp.fc1.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-3.blocks.layer_with_weights-1.mlp.fc1.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-3.blocks.layer_with_weights-1.mlp.fc2.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).basic_layers.layer_with_weights-3.blocks.layer_with_weights-1.mlp.fc2.bias\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)        [(None, 224, 224, 1)]        0         []                            \n",
      "                                                                                                  \n",
      " input_5 (InputLayer)        [(None, 224, 224, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)           (None, 224, 224, 64)         640       ['input_4[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)           (None, 224, 224, 64)         1792      ['input_5[0][0]']             \n",
      "                                                                                                  \n",
      " SwinTransformer (SwinTrans  (None, 768)                  4926709   ['conv2d_2[0][0]',            \n",
      " formerModel)                                             4          'conv2d_3[0][0]']            \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate  (None, 1536)                 0         ['SwinTransformer[0][0]',     \n",
      " )                                                                   'SwinTransformer[1][0]']     \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 2)                    3074      ['concatenate_1[0][0]']       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 49272600 (189.24 MB)\n",
      "Trainable params: 48936460 (186.68 MB)\n",
      "Non-trainable params: 336140 (2.56 MB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape_vsi = (224, 224, 1)  # Replace with actual size\n",
    "input_shape_tda = (224, 224, 3)  # Replace with actual size\n",
    "num_classes = 2  # Healthy or Diseased\n",
    "\n",
    "# Create the model\n",
    "model1 = create_model(input_shape_vsi, input_shape_tda, num_classes, swin_model_type='swin_small_224', pretrained=True)\n",
    "\n",
    "# Compile the model\n",
    "model1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0ec26d-35a5-4b70-bed5-da0ca09955b5",
   "metadata": {},
   "source": [
    "## Implementation 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fbe62f03-7179-49c4-b58f-90442efb4c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_6 (InputLayer)        [(None, 224, 224, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " input_7 (InputLayer)        [(None, 224, 224, 1)]        0         []                            \n",
      "                                                                                                  \n",
      " input_8 (InputLayer)        [(None, 224, 224, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)           (None, 224, 224, 64)         1792      ['input_6[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)           (None, 224, 224, 64)         640       ['input_7[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)           (None, 224, 224, 64)         1792      ['input_8[0][0]']             \n",
      "                                                                                                  \n",
      " SwinTransformer (SwinTrans  (None, 768)                  4931475   ['conv2d_4[0][0]',            \n",
      " formerModel)                                             2          'conv2d_5[0][0]',            \n",
      "                                                                     'conv2d_6[0][0]']            \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate  (None, 2304)                 0         ['SwinTransformer[0][0]',     \n",
      " )                                                                   'SwinTransformer[1][0]',     \n",
      "                                                                     'SwinTransformer[2][0]']     \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 2)                    4610      ['concatenate_2[0][0]']       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 49323586 (189.44 MB)\n",
      "Trainable params: 48987446 (186.87 MB)\n",
      "Non-trainable params: 336140 (2.56 MB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, Dense, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from swin_transformer_final import SwinTransformerModel, CFGS\n",
    "\n",
    "def create_model(input_shape_rgb, input_shape_vsi, input_shape_tda, num_classes, pretrained=True, swin_model_type='swin_tiny_224'):\n",
    "    # Inputs for RGB, VSI, and TDA images\n",
    "    input_rgb = Input(shape=input_shape_rgb)\n",
    "    input_vsi = Input(shape=input_shape_vsi)\n",
    "    input_tda = Input(shape=input_shape_tda)\n",
    "\n",
    "    # Initial Convolutional layer for each input\n",
    "    conv_rgb = Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same')(input_rgb)\n",
    "    conv_vsi = Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same')(input_vsi)\n",
    "    conv_tda = Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same')(input_tda)\n",
    "\n",
    "    # Load the Swin Transformer model configuration\n",
    "    swin_config = CFGS[swin_model_type]\n",
    "    swin_transformer_model = SwinTransformerModel(include_top=False, num_classes=num_classes, **swin_config)\n",
    "    model_name = swin_model_type\n",
    "\n",
    "    ###################################\n",
    "    if pretrained is True:\n",
    "        url = f'https://github.com/rishigami/Swin-Transformer-TF/releases/download/v0.1-tf-swin-weights/{model_name}.tgz'\n",
    "        pretrained_weights = tf.keras.utils.get_file(model_name, url, untar=True)\n",
    "    else:\n",
    "        pretrained_weights = pretrained\n",
    "    \n",
    "    # Load weights into the model if pretrained_weights is provided\n",
    "    if pretrained_weights:\n",
    "        if tf.io.gfile.isdir(pretrained_weights):\n",
    "            pretrained_weights = f'{pretrained_weights}/{model_name}.ckpt'\n",
    "        \n",
    "        \n",
    "    swin_transformer_model.load_weights(pretrained_weights)\n",
    "    #####################################\n",
    "\n",
    "    # Swin Transformer Encoder blocks for RGB, VSI, and TDA images\n",
    "    swin_output_rgb = swin_transformer_model(conv_rgb)\n",
    "    swin_output_vsi = swin_transformer_model(conv_vsi)\n",
    "    swin_output_tda = swin_transformer_model(conv_tda)\n",
    "\n",
    "    # Concatenate the output features from all Swin Transformer encoders\n",
    "    concatenated = Concatenate()([swin_output_rgb, swin_output_vsi, swin_output_tda])\n",
    "\n",
    "    # Classifier head\n",
    "    classifier_output = Dense(num_classes, activation='softmax')(concatenated)\n",
    "\n",
    "    # Create the Model\n",
    "    model = Model(inputs=[input_rgb, input_vsi, input_tda], outputs=classifier_output)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Define input shapes based on the preprocessed data sizes\n",
    "input_shape_rgb = (224, 224, 3)  # Adjust based on your RGB image size\n",
    "input_shape_vsi = (224, 224, 1)  # Replace with actual size for VSI images\n",
    "input_shape_tda = (224, 224, 3)  # Replace with actual size for TDA images\n",
    "num_classes = 2  # Healthy or Diseased\n",
    "\n",
    "# Create the model\n",
    "model = create_model(input_shape_rgb, input_shape_vsi, input_shape_tda, num_classes, swin_model_type='swin_large_224')\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary to verify the architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2d0c05-acf4-4bd1-aeaf-97402dfe8628",
   "metadata": {},
   "source": [
    "## Implementation 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd01e8ef-10f6-4c9c-9901-75c682752737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_rgb (InputLayer)      [(None, 224, 224, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " input_8 (InputLayer)        [(None, 224, 224, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " input_tda (InputLayer)      [(None, 224, 224, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " swin_small_224_encoder_rgb  (None, 1000)                 4994239   ['input_rgb[0][0]']           \n",
      "  (Functional)                                            8                                       \n",
      "                                                                                                  \n",
      " swin_small_224_encoder_vsi  (None, 1000)                 4994239   ['input_8[0][0]']             \n",
      "  (Functional)                                            8                                       \n",
      "                                                                                                  \n",
      " swin_small_224_encoder_tda  (None, 1000)                 4994239   ['input_tda[0][0]']           \n",
      "  (Functional)                                            8                                       \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate  (None, 3000)                 0         ['swin_small_224_encoder_rgb[1\n",
      " )                                                                  ][0]',                        \n",
      "                                                                     'swin_small_224_encoder_vsi[1\n",
      "                                                                    ][0]',                        \n",
      "                                                                     'swin_small_224_encoder_tda[1\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      " classifier (Dense)          (None, 2)                    6002      ['concatenate_1[1][0]']       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 149833196 (575.42 MB)\n",
      "Trainable params: 148824776 (567.72 MB)\n",
      "Non-trainable params: 1008420 (7.69 MB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, Dense, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from swin_transformer_final import SwinTransformerModel, CFGS\n",
    "\n",
    "def create_swin_encoder(input_shape, swin_model_type, pretrained=True, layer_img='rgb'):\n",
    "    # Load the Swin Transformer model configuration\n",
    "    swin_config = CFGS[swin_model_type]\n",
    "    swin_transformer = SwinTransformerModel(include_top=False, **swin_config)\n",
    "    \n",
    "    # Load pretrained weights if specified\n",
    "    if pretrained:\n",
    "        model_name = swin_model_type\n",
    "        url = f'https://github.com/rishigami/Swin-Transformer-TF/releases/download/v0.1-tf-swin-weights/{model_name}.tgz'\n",
    "        pretrained_weights = tf.keras.utils.get_file(fname=model_name, origin=url, untar=True)\n",
    "        if tf.io.gfile.isdir(pretrained_weights):\n",
    "            pretrained_weights = f'{pretrained_weights}/{model_name}.ckpt'\n",
    "        swin_transformer.load_weights(pretrained_weights)\n",
    "    \n",
    "    # Define the input layer\n",
    "    input_layer = Input(shape=input_shape)\n",
    "\n",
    "    # Forward pass up to the global average pooling\n",
    "    x = swin_transformer.patch_embedding(input_layer)\n",
    "    x = swin_transformer.basic_layers(x)\n",
    "    x = swin_transformer.normalization_layer(x)\n",
    "    x = swin_transformer.global_average_pooling(x)\n",
    "\n",
    "    # If the model does not output a 1000-dimensional vector, use a Dense layer to project it\n",
    "    if swin_transformer.n_features != 1000:\n",
    "        x = Dense(1000)(x)\n",
    "\n",
    "    # Create a model\n",
    "    return Model(input_layer, x, name=f'{swin_model_type}_encoder_{layer_img}')\n",
    "\n",
    "\n",
    "# Define input shapes\n",
    "input_shape_rgb = (224, 224, 3)\n",
    "input_shape_vsi = (224, 224, 3)\n",
    "input_shape_tda = (224, 224, 3)\n",
    "\n",
    "# Create Swin Transformer encoders for each input type\n",
    "swin_encoder_rgb = create_swin_encoder(input_shape_rgb, 'swin_small_224', layer_img='rgb')\n",
    "swin_encoder_vsi = create_swin_encoder(input_shape_vsi, 'swin_small_224', layer_img='vsi')\n",
    "swin_encoder_tda = create_swin_encoder(input_shape_tda, 'swin_small_224', layer_img='tda')\n",
    "\n",
    "# Inputs for RGB, VSI, and TDA images\n",
    "input_rgb = Input(shape=input_shape_rgb, name='input_rgb')\n",
    "input_vsi = Input(shape=(224,224,1), name='input_vsi')\n",
    "input_tda = Input(shape=input_shape_tda, name='input_tda')\n",
    "\n",
    "input_after_conv_vsi = Conv2D(3, kernel_size=(3,3), padding='same', activation='relu')(input_vsi)\n",
    "\n",
    "# Process each input through its respective Swin Transformer encoder\n",
    "features_rgb = swin_encoder_rgb(input_rgb)\n",
    "features_vsi = swin_encoder_vsi(input_after_conv_vsi)\n",
    "features_tda = swin_encoder_tda(input_tda)\n",
    "\n",
    "# Concatenate the output features from all Swin Transformer encoders\n",
    "concatenated_features = Concatenate()([features_rgb, features_vsi, features_tda])\n",
    "\n",
    "# Classifier head\n",
    "classifier_output = Dense(2, activation='softmax', name='classifier')(concatenated_features)\n",
    "\n",
    "# Create the combined Model\n",
    "combined_model = Model(inputs=[input_rgb, input_after_conv_vsi, input_tda], outputs=classifier_output)\n",
    "\n",
    "# Compile the combined model\n",
    "combined_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Summary of the model\n",
    "combined_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f52516e-fdba-4151-a6dd-3849bceb8086",
   "metadata": {},
   "source": [
    "# Implementation with just RGB + VSI Patched Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01ce04b5-0026-4dfe-84d3-b19a8fb98063",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-12 23:13:56.448146: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
      "2024-04-12 23:13:56.448162: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2024-04-12 23:13:56.448170: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2024-04-12 23:13:56.448196: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-04-12 23:13:56.448213: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, Dense, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from swin_transformer_final import SwinTransformerModel, CFGS\n",
    "\n",
    "def create_swin_encoder(input_shape, swin_model_type, pretrained=True, layer_img='rgb'):\n",
    "    # Load the Swin Transformer model configuration\n",
    "    swin_config = CFGS[swin_model_type]\n",
    "    swin_transformer = SwinTransformerModel(include_top=False, **swin_config)\n",
    "    \n",
    "    # Load pretrained weights if specified\n",
    "    if pretrained:\n",
    "        model_name = swin_model_type\n",
    "        url = f'https://github.com/rishigami/Swin-Transformer-TF/releases/download/v0.1-tf-swin-weights/{model_name}.tgz'\n",
    "        pretrained_weights = tf.keras.utils.get_file(fname=model_name, origin=url, untar=True)\n",
    "        if tf.io.gfile.isdir(pretrained_weights):\n",
    "            pretrained_weights = f'{pretrained_weights}/{model_name}.ckpt'\n",
    "        swin_transformer.load_weights(pretrained_weights)\n",
    "    \n",
    "    # Define the input layer\n",
    "    input_layer = Input(shape=input_shape)\n",
    "\n",
    "    # Forward pass up to the global average pooling\n",
    "    x = swin_transformer.patch_embedding(input_layer)\n",
    "    x = swin_transformer.basic_layers(x)\n",
    "    x = swin_transformer.normalization_layer(x)\n",
    "    x = swin_transformer.global_average_pooling(x)\n",
    "\n",
    "    # If the model does not output a 1000-dimensional vector, use a Dense layer to project it\n",
    "    if swin_transformer.n_features != 1000:\n",
    "        x = Dense(1000)(x)\n",
    "\n",
    "    # Create a model\n",
    "    return Model(input_layer, x, name=f'{swin_model_type}_encoder_{layer_img}')\n",
    "\n",
    "\n",
    "# Define input shapes\n",
    "input_shape_rgb = (224, 224, 3)\n",
    "input_shape_vsi = (224, 224, 3)\n",
    "# input_shape_tda = (224, 224, 3)\n",
    "\n",
    "# Create Swin Transformer encoders for each input type\n",
    "swin_encoder_rgb = create_swin_encoder(input_shape_rgb, 'swin_tiny_224', layer_img='rgb')\n",
    "swin_encoder_vsi = create_swin_encoder(input_shape_vsi, 'swin_tiny_224', layer_img='vsi')\n",
    "# swin_encoder_tda = create_swin_encoder(input_shape_tda, 'swin_small_224', layer_img='tda')\n",
    "\n",
    "# Inputs for RGB, VSI, and TDA images\n",
    "input_rgb = Input(shape=input_shape_rgb, name='input_rgb')\n",
    "input_vsi = Input(shape=(224,224,1), name='input_vsi')\n",
    "# input_tda = Input(shape=input_shape_tda, name='input_tda')\n",
    "\n",
    "input_after_conv_vsi = Conv2D(3, kernel_size=(3,3), padding='same', activation='relu')(input_vsi)\n",
    "\n",
    "# Process each input through its respective Swin Transformer encoder\n",
    "features_rgb = swin_encoder_rgb(input_rgb)\n",
    "features_vsi = swin_encoder_vsi(input_after_conv_vsi)\n",
    "# features_tda = swin_encoder_tda(input_tda)\n",
    "\n",
    "# Concatenate the output features from all Swin Transformer encoders\n",
    "concatenated_features = Concatenate()([features_rgb, features_vsi]) # , features_tda\n",
    "\n",
    "# Classifier head\n",
    "classifier_output = Dense(2, activation='softmax', name='classifier')(concatenated_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e974f23a-c312-4c2a-a6c1-2d0d94c315a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in swin_encoder_rgb.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Freeze the Swin Transformer layers for VSI encoder\n",
    "for layer in swin_encoder_vsi.layers[:-1]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c82995e-b510-4b58-9858-b031236a1fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_vsi (InputLayer)      [(None, 224, 224, 1)]        0         []                            \n",
      "                                                                                                  \n",
      " input_rgb (InputLayer)      [(None, 224, 224, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)             (None, 224, 224, 3)          30        ['input_vsi[0][0]']           \n",
      "                                                                                                  \n",
      " swin_tiny_224_encoder_rgb   (None, 1000)                 2853805   ['input_rgb[0][0]']           \n",
      " (Functional)                                             8                                       \n",
      "                                                                                                  \n",
      " swin_tiny_224_encoder_vsi   (None, 1000)                 2853805   ['conv2d[0][0]']              \n",
      " (Functional)                                             8                                       \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 2000)                 0         ['swin_tiny_224_encoder_rgb[0]\n",
      "                                                                    [0]',                         \n",
      "                                                                     'swin_tiny_224_encoder_vsi[0]\n",
      "                                                                    [0]']                         \n",
      "                                                                                                  \n",
      " classifier (Dense)          (None, 2)                    4002      ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 57080148 (219.65 MB)\n",
      "Trainable params: 1542032 (5.88 MB)\n",
      "Non-trainable params: 55538116 (213.77 MB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create the combined Model\n",
    "combined_model = Model(inputs=[input_rgb, input_vsi], outputs=classifier_output) # , input_tda\n",
    "\n",
    "# Compile the combined model\n",
    "combined_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Summary of the model\n",
    "combined_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e7cb608-9530-4835-a366-f95dc2b9d223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_vsi True\n",
      "input_rgb True\n",
      "conv2d True\n",
      "swin_tiny_224_encoder_rgb True\n",
      "swin_tiny_224_encoder_vsi True\n",
      "concatenate True\n",
      "classifier True\n"
     ]
    }
   ],
   "source": [
    "# Verify that the layers are frozen\n",
    "for layer in combined_model.layers:\n",
    "    print(layer.name, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3aa64f39-b3c0-4832-9e22-85a7e1a090cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable status of layers in swin_encoder_rgb:\n",
      "input_1 False\n",
      "patch_embed False\n",
      "sequential_4 False\n",
      "norm False\n",
      "global_average_pooling1d False\n",
      "dense True\n",
      "\n",
      "Trainable status of layers in swin_encoder_vsi:\n",
      "input_2 False\n",
      "patch_embed False\n",
      "sequential_9 False\n",
      "norm False\n",
      "global_average_pooling1d_1 False\n",
      "dense_1 True\n"
     ]
    }
   ],
   "source": [
    "# Check the trainable status of the layers in swin_encoder_rgb\n",
    "print(\"Trainable status of layers in swin_encoder_rgb:\")\n",
    "for layer in swin_encoder_rgb.layers:\n",
    "    print(layer.name, layer.trainable)\n",
    "\n",
    "# Check the trainable status of the layers in swin_encoder_vsi\n",
    "print(\"\\nTrainable status of layers in swin_encoder_vsi:\")\n",
    "for layer in swin_encoder_vsi.layers:\n",
    "    print(layer.name, layer.trainable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3029580-f1c4-4ba7-91a1-244fe10790b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "train_rgb, train_labels = np.load('./training_data/images_rgb.npy'), np.load('./training_data/image_labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b290a727-4e79-4088-a84b-249ac3943715",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vessels, train_labels_v = np.load('./training_data/vessels_seg.npy'), np.load('./training_data/vessels_seg.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4549bb1-fe8a-4765-94ab-2b6f33c05c7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3150, 224, 224, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_rgb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6aa3b909-3050-4ded-865f-b9c09ce8bfae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3150, 224, 224)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vessels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2babec98-325c-4b69-8c56-dbe3f91a2b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vessels = train_vessels.reshape(3150,224,224,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e65d0fa-5c93-4bf7-99ce-f31e3a999a60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3150, 224, 224, 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vessels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b75134fd-671f-43b2-93bd-1acfa0190094",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14bad352-080e-4b15-a282-566069d6551e",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_training = [train_rgb, train_vessels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ba3417f-8b8f-42d0-9983-92ac3c49a004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224, 224, 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vessels[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b66fd16c-5f03-42f3-b740-26b018deeca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-12 23:14:53.369578: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99/99 [==============================] - 258s 2s/step - loss: 1.2522 - accuracy: 0.6562\n",
      "Epoch 2/5\n",
      "99/99 [==============================] - 223s 2s/step - loss: 0.5734 - accuracy: 0.7321\n",
      "Epoch 3/5\n",
      "99/99 [==============================] - 236s 2s/step - loss: 0.5827 - accuracy: 0.7298\n",
      "Epoch 4/5\n",
      "99/99 [==============================] - 252s 3s/step - loss: 0.4978 - accuracy: 0.7679\n",
      "Epoch 5/5\n",
      "99/99 [==============================] - 242s 2s/step - loss: 0.6245 - accuracy: 0.7105\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x28bbc2590>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_model.fit(final_training, train_labels, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f1d73b8-ec99-4759-bcd5-e519f1c6b99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99/99 [==============================] - 134s 1s/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = combined_model.predict(final_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "506a06d8-20b6-4e37-8cbc-d0e99fab04b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('y_pred_rgb_vsi.npy', y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ec9c757-6b7c-4366-a8af-38d9939f37a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "roc = roc_curve(train_labels, y_pred[:,1])\n",
    "fpr, tpr = roc[0], roc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591bc651-b664-4c78-92f7-6132ccc596ef",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4b7ca1-5b6e-4238-8b24-fd975bee8471",
   "metadata": {},
   "source": [
    "# All Channels - RGB + VSI + TDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ff86fe7-5b57-465e-b1d1-372139680063",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-27 14:41:20.283247: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
      "2024-04-27 14:41:20.283265: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2024-04-27 14:41:20.283269: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2024-04-27 14:41:20.283295: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-04-27 14:41:20.283306: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, Dense, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from swin_transformer_final import SwinTransformerModel, CFGS\n",
    "\n",
    "def create_swin_encoder(input_shape, swin_model_type, pretrained=True, layer_img='rgb'):\n",
    "    # Load the Swin Transformer model configuration\n",
    "    swin_config = CFGS[swin_model_type]\n",
    "    swin_transformer = SwinTransformerModel(include_top=False, **swin_config)\n",
    "    \n",
    "    # Load pretrained weights if specified\n",
    "    if pretrained:\n",
    "        model_name = swin_model_type\n",
    "        url = f'https://github.com/rishigami/Swin-Transformer-TF/releases/download/v0.1-tf-swin-weights/{model_name}.tgz'\n",
    "        pretrained_weights = tf.keras.utils.get_file(fname=model_name, origin=url, untar=True)\n",
    "        if tf.io.gfile.isdir(pretrained_weights):\n",
    "            pretrained_weights = f'{pretrained_weights}/{model_name}.ckpt'\n",
    "        swin_transformer.load_weights(pretrained_weights)\n",
    "    \n",
    "    # Define the input layer\n",
    "    input_layer = Input(shape=input_shape)\n",
    "\n",
    "    # Forward pass up to the global average pooling\n",
    "    x = swin_transformer.patch_embedding(input_layer)\n",
    "    x = swin_transformer.basic_layers(x)\n",
    "    x = swin_transformer.normalization_layer(x)\n",
    "    x = swin_transformer.global_average_pooling(x)\n",
    "\n",
    "    # If the model does not output a 1000-dimensional vector, use a Dense layer to project it\n",
    "    if swin_transformer.n_features != 1000:\n",
    "        x = Dense(1000)(x)\n",
    "\n",
    "    # Create a model\n",
    "    return Model(input_layer, x, name=f'{swin_model_type}_encoder_{layer_img}')\n",
    "\n",
    "\n",
    "# Define input shapes\n",
    "input_shape_rgb = (224, 224, 3)\n",
    "input_shape_vsi = (224, 224, 3)\n",
    "input_shape_tda = (224, 224, 3)\n",
    "\n",
    "# Create Swin Transformer encoders for each input type\n",
    "swin_encoder_rgb = create_swin_encoder(input_shape_rgb, 'swin_tiny_224', layer_img='rgb')\n",
    "swin_encoder_vsi = create_swin_encoder(input_shape_vsi, 'swin_tiny_224', layer_img='vsi')\n",
    "swin_encoder_tda = create_swin_encoder(input_shape_tda, 'swin_tiny_224', layer_img='tda')\n",
    "\n",
    "# Inputs for RGB, VSI, and TDA images\n",
    "input_rgb = Input(shape=input_shape_rgb, name='input_rgb')\n",
    "input_vsi = Input(shape=(224,224,1), name='input_vsi')\n",
    "input_tda = Input(shape=input_shape_tda, name='input_tda')\n",
    "\n",
    "input_after_conv_vsi = Conv2D(3, kernel_size=(3,3), padding='same', activation='relu')(input_vsi)\n",
    "\n",
    "# Process each input through its respective Swin Transformer encoder\n",
    "features_rgb = swin_encoder_rgb(input_rgb)\n",
    "features_vsi = swin_encoder_vsi(input_after_conv_vsi)\n",
    "features_tda = swin_encoder_tda(input_tda)\n",
    "\n",
    "# Concatenate the output features from all Swin Transformer encoders\n",
    "concatenated_features = Concatenate()([features_rgb, features_vsi, features_tda])\n",
    "\n",
    "# Classifier head\n",
    "classifier_output = Dense(2, activation='softmax', name='classifier')(concatenated_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ebc7b5b-d6fa-449a-a488-83572f99409b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in swin_encoder_rgb.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Freeze the Swin Transformer layers for VSI encoder\n",
    "for layer in swin_encoder_vsi.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "\n",
    "for layer in swin_encoder_tda.layers[:-1]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22dcdd2a-5135-4558-957a-203097711ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable status of layers in swin_encoder_rgb:\n",
      "input_1 False\n",
      "patch_embed False\n",
      "sequential_4 False\n",
      "norm False\n",
      "global_average_pooling1d False\n",
      "dense True\n",
      "\n",
      "Trainable status of layers in swin_encoder_vsi:\n",
      "input_2 False\n",
      "patch_embed False\n",
      "sequential_9 False\n",
      "norm False\n",
      "global_average_pooling1d_1 False\n",
      "dense_1 True\n",
      "\n",
      "Trainable status of layers in swin_encoder_tda:\n",
      "input_3 False\n",
      "patch_embed False\n",
      "sequential_14 False\n",
      "norm False\n",
      "global_average_pooling1d_2 False\n",
      "dense_2 True\n"
     ]
    }
   ],
   "source": [
    "# Check the trainable status of the layers in swin_encoder_rgb\n",
    "print(\"Trainable status of layers in swin_encoder_rgb:\")\n",
    "for layer in swin_encoder_rgb.layers:\n",
    "    print(layer.name, layer.trainable)\n",
    "\n",
    "# Check the trainable status of the layers in swin_encoder_vsi\n",
    "print(\"\\nTrainable status of layers in swin_encoder_vsi:\")\n",
    "for layer in swin_encoder_vsi.layers:\n",
    "    print(layer.name, layer.trainable)\n",
    "\n",
    "print(\"\\nTrainable status of layers in swin_encoder_tda:\")\n",
    "for layer in swin_encoder_tda.layers:\n",
    "    print(layer.name, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be827877-6783-409c-87fc-6a5872c91993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_vsi (InputLayer)      [(None, 224, 224, 1)]        0         []                            \n",
      "                                                                                                  \n",
      " input_rgb (InputLayer)      [(None, 224, 224, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)             (None, 224, 224, 3)          30        ['input_vsi[0][0]']           \n",
      "                                                                                                  \n",
      " input_tda (InputLayer)      [(None, 224, 224, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " swin_tiny_224_encoder_rgb   (None, 1000)                 2853805   ['input_rgb[0][0]']           \n",
      " (Functional)                                             8                                       \n",
      "                                                                                                  \n",
      " swin_tiny_224_encoder_vsi   (None, 1000)                 2853805   ['conv2d[0][0]']              \n",
      " (Functional)                                             8                                       \n",
      "                                                                                                  \n",
      " swin_tiny_224_encoder_tda   (None, 1000)                 2853805   ['input_tda[0][0]']           \n",
      " (Functional)                                             8                                       \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 3000)                 0         ['swin_tiny_224_encoder_rgb[0]\n",
      "                                                                    [0]',                         \n",
      "                                                                     'swin_tiny_224_encoder_vsi[0]\n",
      "                                                                    [0]',                         \n",
      "                                                                     'swin_tiny_224_encoder_tda[0]\n",
      "                                                                    [0]']                         \n",
      "                                                                                                  \n",
      " classifier (Dense)          (None, 2)                    6002      ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 85620206 (329.47 MB)\n",
      "Trainable params: 2313032 (8.82 MB)\n",
      "Non-trainable params: 83307174 (320.65 MB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create the combined Model\n",
    "combined_model = Model(inputs=[input_rgb, input_vsi, input_tda], outputs=classifier_output) # , input_tda\n",
    "\n",
    "# Compile the combined model\n",
    "combined_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Summary of the model\n",
    "combined_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "531bca53-c99d-45c5-8b6d-9396e636e17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tda = np.load('./training_data/tda_vr.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c857cf36-41fa-429f-9deb-cf006d119b68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3150, 224, 224, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tda.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b9f76c0-03b5-4cf6-8ef5-a40665bc2947",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_final = [train_rgb, train_vessels, train_tda]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "04c00dd4-ccde-4426-972f-b9b63510d87a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-27 14:42:45.485614: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99/99 [==============================] - 414s 4s/step - loss: 2.1077 - accuracy: 0.6321\n",
      "Epoch 2/8\n",
      "99/99 [==============================] - 417s 4s/step - loss: 0.6206 - accuracy: 0.6775\n",
      "Epoch 3/8\n",
      "99/99 [==============================] - 415s 4s/step - loss: 0.6487 - accuracy: 0.6695\n",
      "Epoch 4/8\n",
      "99/99 [==============================] - 408s 4s/step - loss: 0.5577 - accuracy: 0.7140\n",
      "Epoch 5/8\n",
      "99/99 [==============================] - 423s 4s/step - loss: 0.5673 - accuracy: 0.7114\n",
      "Epoch 6/8\n",
      "99/99 [==============================] - 410s 4s/step - loss: 0.5342 - accuracy: 0.7286\n",
      "Epoch 7/8\n",
      "99/99 [==============================] - 448s 5s/step - loss: 0.5216 - accuracy: 0.7429\n",
      "Epoch 8/8\n",
      "99/99 [==============================] - 454s 5s/step - loss: 0.5170 - accuracy: 0.7470\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x347093730>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_model.fit(training_final, train_labels, epochs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c14c2ad8-0a83-45e2-bf8b-c8559fb333c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3114f5b1-a4c6-4655-8c80-b8352cdcb14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99/99 [==============================] - 254s 2s/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = combined_model.predict(training_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73fc035-ce65-4950-9a6e-867e7cb7a091",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dc2b1f08-fafd-440a-b044-ddc2623ffa4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('y_pred_all_channels_vr.npy', y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d9313ffa-c225-47e8-b891-4d995b77eca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_cb_3 = np.load('y_pred_all_channels_cb.npy')\n",
    "y_pred_vr_3 = np.load('y_pred_all_channels_vr.npy')\n",
    "y_pred2 = np.load('y_pred_rgb_vsi.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "003cbccd-e0ad-4d63-9fd3-d5bb88ce3270",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc = roc_curve(train_labels, y_pred2[:,1])\n",
    "roc_cb = roc_curve(train_labels, y_pred_cb_3[:,1])\n",
    "roc_vr = roc_curve(train_labels, y_pred_vr_3[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6def41e1-4feb-4c31-babe-c7e80e5f36f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr = roc[0], roc[1]\n",
    "fpr_vr, tpr_vr = roc_vr[0], roc_vr[1]\n",
    "fpr_cb, tpr_cb = roc_cb[0], roc_cb[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "716f8150-ba32-407d-b86b-fc0fd0e3213e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(fpr, tpr, label = 'SWIN-T on RGB+VSI')\n",
    "plt.plot(fpr_vr, tpr_vr, label = 'SWIN-T on RGB+VSI+TDA - VRP')\n",
    "plt.plot(fpr_cb, tpr_cb, label = 'SWIN-T on RGB+VSI+TDA - CBP')\n",
    "plt.plot([0,1],[0,1], label='Naive Classifier')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.legend()\n",
    "plt.title('ROC Curve')\n",
    "plt.savefig('ROC.jpeg', dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aeeb9deb-3169-45b4-920d-6892c05fe735",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69463292-7d36-4540-a774-380415919ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_all_channels = np.load('y_pred_all_channels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "17808dac-610e-47d0-a75f-8131815cb0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "def split_and_majority(data):\n",
    "    # Splitting the array into chunks of size 70\n",
    "    n = 70\n",
    "    mini_arrays = [data[i:i + n] for i in range(0, len(data), n)]\n",
    "    \n",
    "    # Calculating the majority for each mini array\n",
    "    majority = [1 if np.sum(chunk) > 69 else 0 for chunk in mini_arrays]\n",
    "    \n",
    "    return majority\n",
    "\n",
    "# Example array\n",
    "data = train_labels  # Replace this with your actual data array\n",
    "\n",
    "# Call the function\n",
    "result = split_and_majority(data)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ddfa30ba-7662-4fb3-a822-4bacabd129c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_int = np.argmax(y_pred_all_channels, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c36c0d75-5322-4efc-b665-dbc01222ce64",
   "metadata": {},
   "outputs": [],
   "source": [
    "result1 = split_and_majority(y_pred_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "db08c7f5-c83f-4d03-a8d6-d1307cc35935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e0ff5c61-0784-4624-808f-b2636996d336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0bb53510-1958-4140-83ce-5e967d070832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3116"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y_pred_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6f346f06-bf8e-4304-b1dd-ee9872cbb710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02717963, 0.9728204 ], dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_all_channels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9f926522-0406-4962-92cb-c4c6fe5a21ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.array(result)==np.array(result1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "11e637af-cf3c-4323-8024-ec27133631ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7777777777777778"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "35/45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b1d0d033-937e-440b-872c-fa693a0d56e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b98acbca-682f-4165-9bdd-e046ed19fb72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8645315192743763"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(train_labels, y_pred_all_channels[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c880024-2d7e-40d2-9611-50e3ef6272f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
