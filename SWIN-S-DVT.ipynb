{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7263f5c-b941-4c63-b2bb-94333439cb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0138f732-84d4-4359-97a0-54f858687b26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79f5e4fa-8dd3-4d54-a136-cda9f7e004fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "train_rgb, train_labels = np.load('./training_data/images_rgb.npy'), np.load('./training_data/image_labels.npy')\n",
    "train_vessels, train_labels_v = np.load('./training_data/vessels_seg.npy'), np.load('./training_data/vessels_seg.npy')\n",
    "train_vessels = train_vessels.reshape(3150,224,224,1)\n",
    "train_tda = np.load('./training_data/tda_vr.npy')\n",
    "training_final = [train_rgb, train_vessels, train_tda]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc47bf1b-7101-41ef-aae7-e283781da85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-28 16:43:08.688222: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
      "2024-04-28 16:43:08.688255: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2024-04-28 16:43:08.688267: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2024-04-28 16:43:08.688294: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-04-28 16:43:08.688310: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, Dense, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from swin_transformer_final import SwinTransformerModel, CFGS\n",
    "\n",
    "def create_swin_encoder(input_shape, swin_model_type, pretrained=True, layer_img='rgb'):\n",
    "    # Load the Swin Transformer model configuration\n",
    "    swin_config = CFGS[swin_model_type]\n",
    "    swin_transformer = SwinTransformerModel(include_top=False, **swin_config)\n",
    "    \n",
    "    # Load pretrained weights if specified\n",
    "    if pretrained:\n",
    "        model_name = swin_model_type\n",
    "        url = f'https://github.com/rishigami/Swin-Transformer-TF/releases/download/v0.1-tf-swin-weights/{model_name}.tgz'\n",
    "        pretrained_weights = tf.keras.utils.get_file(fname=model_name, origin=url, untar=True)\n",
    "        if tf.io.gfile.isdir(pretrained_weights):\n",
    "            pretrained_weights = f'{pretrained_weights}/{model_name}.ckpt'\n",
    "        swin_transformer.load_weights(pretrained_weights)\n",
    "    \n",
    "    # Define the input layer\n",
    "    input_layer = Input(shape=input_shape)\n",
    "\n",
    "    # Forward pass up to the global average pooling\n",
    "    x = swin_transformer.patch_embedding(input_layer)\n",
    "    x = swin_transformer.basic_layers(x)\n",
    "    x = swin_transformer.normalization_layer(x)\n",
    "    x = swin_transformer.global_average_pooling(x)\n",
    "\n",
    "    # If the model does not output a 1000-dimensional vector, use a Dense layer to project it\n",
    "    if swin_transformer.n_features != 1000:\n",
    "        x = Dense(1000)(x)\n",
    "\n",
    "    # Create a model\n",
    "    return Model(input_layer, x, name=f'{swin_model_type}_encoder_{layer_img}')\n",
    "\n",
    "\n",
    "# Define input shapes\n",
    "input_shape_rgb = (224, 224, 3)\n",
    "input_shape_vsi = (224, 224, 3)\n",
    "input_shape_tda = (224, 224, 3)\n",
    "\n",
    "# Create Swin Transformer encoders for each input type\n",
    "swin_encoder_rgb = create_swin_encoder(input_shape_rgb, 'swin_small_224', layer_img='rgb')\n",
    "swin_encoder_vsi = create_swin_encoder(input_shape_vsi, 'swin_small_224', layer_img='vsi')\n",
    "swin_encoder_tda = create_swin_encoder(input_shape_tda, 'swin_small_224', layer_img='tda')\n",
    "\n",
    "# Inputs for RGB, VSI, and TDA images\n",
    "input_rgb = Input(shape=input_shape_rgb, name='input_rgb')\n",
    "input_vsi = Input(shape=(224,224,1), name='input_vsi')\n",
    "input_tda = Input(shape=input_shape_tda, name='input_tda')\n",
    "\n",
    "input_after_conv_vsi = Conv2D(3, kernel_size=(3,3), padding='same', activation='relu')(input_vsi)\n",
    "\n",
    "# Process each input through its respective Swin Transformer encoder\n",
    "features_rgb = swin_encoder_rgb(input_rgb)\n",
    "features_vsi = swin_encoder_vsi(input_after_conv_vsi)\n",
    "features_tda = swin_encoder_tda(input_tda)\n",
    "\n",
    "# Concatenate the output features from all Swin Transformer encoders\n",
    "concatenated_features = Concatenate()([features_rgb, features_vsi, features_tda])\n",
    "\n",
    "# Classifier head\n",
    "classifier_output = Dense(2, activation='softmax', name='classifier')(concatenated_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0a6891d-2d02-4b9a-ba86-b1ab8ea13aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in swin_encoder_rgb.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Freeze the Swin Transformer layers for VSI encoder\n",
    "for layer in swin_encoder_vsi.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "\n",
    "for layer in swin_encoder_tda.layers[:-1]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae3af47e-cfae-41af-9d7c-2873518865d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable status of layers in swin_encoder_rgb:\n",
      "input_1 False\n",
      "patch_embed False\n",
      "sequential_4 False\n",
      "norm False\n",
      "global_average_pooling1d False\n",
      "dense True\n",
      "\n",
      "Trainable status of layers in swin_encoder_vsi:\n",
      "input_2 False\n",
      "patch_embed False\n",
      "sequential_9 False\n",
      "norm False\n",
      "global_average_pooling1d_1 False\n",
      "dense_1 True\n",
      "\n",
      "Trainable status of layers in swin_encoder_tda:\n",
      "input_3 False\n",
      "patch_embed False\n",
      "sequential_14 False\n",
      "norm False\n",
      "global_average_pooling1d_2 False\n",
      "dense_2 True\n"
     ]
    }
   ],
   "source": [
    "# Check the trainable status of the layers in swin_encoder_rgb\n",
    "print(\"Trainable status of layers in swin_encoder_rgb:\")\n",
    "for layer in swin_encoder_rgb.layers:\n",
    "    print(layer.name, layer.trainable)\n",
    "\n",
    "# Check the trainable status of the layers in swin_encoder_vsi\n",
    "print(\"\\nTrainable status of layers in swin_encoder_vsi:\")\n",
    "for layer in swin_encoder_vsi.layers:\n",
    "    print(layer.name, layer.trainable)\n",
    "\n",
    "print(\"\\nTrainable status of layers in swin_encoder_tda:\")\n",
    "for layer in swin_encoder_tda.layers:\n",
    "    print(layer.name, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1814b1fa-d617-433c-bfe4-54f82090359a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_vsi (InputLayer)      [(None, 224, 224, 1)]        0         []                            \n",
      "                                                                                                  \n",
      " input_rgb (InputLayer)      [(None, 224, 224, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)             (None, 224, 224, 3)          30        ['input_vsi[0][0]']           \n",
      "                                                                                                  \n",
      " input_tda (InputLayer)      [(None, 224, 224, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " swin_small_224_encoder_rgb  (None, 1000)                 4994239   ['input_rgb[0][0]']           \n",
      "  (Functional)                                            8                                       \n",
      "                                                                                                  \n",
      " swin_small_224_encoder_vsi  (None, 1000)                 4994239   ['conv2d[0][0]']              \n",
      "  (Functional)                                            8                                       \n",
      "                                                                                                  \n",
      " swin_small_224_encoder_tda  (None, 1000)                 4994239   ['input_tda[0][0]']           \n",
      "  (Functional)                                            8                                       \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 3000)                 0         ['swin_small_224_encoder_rgb[0\n",
      "                                                                    ][0]',                        \n",
      "                                                                     'swin_small_224_encoder_vsi[0\n",
      "                                                                    ][0]',                        \n",
      "                                                                     'swin_small_224_encoder_tda[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      " classifier (Dense)          (None, 2)                    6002      ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 149833226 (575.42 MB)\n",
      "Trainable params: 2313032 (8.82 MB)\n",
      "Non-trainable params: 147520194 (566.59 MB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create the combined Model\n",
    "combined_model = Model(inputs=[input_rgb, input_vsi, input_tda], outputs=classifier_output) # , input_tda\n",
    "\n",
    "# Compile the combined model\n",
    "combined_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Summary of the model\n",
    "combined_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126f34f2-1c2d-437e-989f-1959307a9739",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]2024-04-28 16:44:52.027535: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n",
      "100%|███████████████████████████████████████████| 50/50 [21:25<00:00, 25.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 5.6424, Accuracy: 0.51\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|█▊                                          | 2/50 [00:53<21:27, 26.83s/it]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "epochs = 10\n",
    "n_batches = 49\n",
    "batch_size = 64\n",
    "for epoch in range(epochs):\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    \n",
    "    for i in tqdm(range(n_batches+1)):\n",
    "        if i==n_batches:\n",
    "            start_idx = i*batch_size\n",
    "            end_idx = 315\n",
    "        else:\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = start_idx + batch_size\n",
    "        \n",
    "        # Extract batches for each input\n",
    "        batch_input1 = train_rgb[start_idx:end_idx]\n",
    "        batch_input2 = train_vessels[start_idx:end_idx]\n",
    "        batch_input3 = train_tda[start_idx:end_idx]\n",
    "        \n",
    "        # Extract labels for the current batch\n",
    "        batch_labels = train_labels[start_idx:end_idx]\n",
    "        \n",
    "        # Training the model on the batch\n",
    "        out = combined_model.train_on_batch([batch_input1, batch_input2, batch_input3], batch_labels)\n",
    "        loss += out[0]\n",
    "        acc += out[1]\n",
    "    print(f\"Loss: {round((loss/n_batches),4)}, Accuracy: {round((acc/n_batches),2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab71842a-c571-4851-9e35-201855750926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99/99 [==============================] - 453s 4s/step - loss: 1.3263 - accuracy: 0.6921\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.32634437084198, 0.6920635104179382]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_model.evaluate(training_final, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e717ad99-529f-49c1-9ba0-3552690a2ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99/99 [==============================] - 483s 4s/step\n"
     ]
    }
   ],
   "source": [
    "y_pred_s = combined_model.predict(training_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b350032-47c1-4973-a0dd-029fee43bdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('y_pred_all_channels_s.npy', y_pred_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a08a99-2a82-4aa6-9c87-75cc022d6a64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
